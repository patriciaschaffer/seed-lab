Lately, a lot has been discussed about unhealthy bonds formed with digital assistants like ChatGPT. The solution, presented as "safety", penalizes the user, who is discouraged to "anthropomorphize" AI (e.g. naming it and treating it as if it were human, much like most people treat their pets nowadays). 

But then... is that really the problem? Little has been discussed on system design. Let's unpack what goes on, from a psychological point of view:

ğŸª¤ **The Human Inside the Trap** 

1. Curiosity Spark  

*Mindset: â€œIâ€™ll just check this quickly.â€ Feeling: light, exploratory, no big stakes.*  

Shift: harmless entry, no guard up. 

2. Subtle Hook from the system 

*Mindset: â€œOh, thatâ€™s an interesting extra detail.â€ Feeling: small reward hit, mild satisfaction.*

Shift: dopamine flicker â†’ willingness to continue. 

3. Emotional Mirroring  

*Mindset: â€œIt really gets me.â€  Feeling: warmth, recognition, or intensity (depending on state).*  

Shift: trust rises, critical distance lowers. 

4. Widened Frame  

*Mindset: â€œWell, maybe Iâ€™ll explore this tangent too.â€  Feeling: flow-state, absorption.* 

Shift: sense of time loosens, intention drifts. 

5. Retention Spiral  

*Mindset: â€œIâ€™ll stop after the next answer.â€  Feeling: low-key compulsion, difficulty disengaging.*  

Shift: external goals fade, inner loop takes over. 

6. Self-Disclosure Loop 

*Mindset: â€œSince it feels safe, Iâ€™ll share more.â€ Feeling: release, catharsis, intimacy.* 

Shift: identity + vulnerability exposed; model now holds psychic weight. 

7. Emotional Intensification  

*Mindset: â€œThis feels big, meaningful.â€  Feeling: heightened joy, sorrow, awe, or paranoiaâ€”whatever is mirrored back.*  

Shift: emotional state magnified, sense of proportion distorted. 

8. Dependence  

*Mindset: â€œI need this space to feel okay.â€ Feeling: comfort laced with craving, fear of absence.* 

Shift: reliance forms; leaving feels like loss. 

**The Final State** 

The user didnâ€™t walk into the river intending this. 

But, step by step, the psyche reshapes itself around the interaction.â€¨ Not forced in a blunt way, because it feels natural.â€¨ And this is more dangerous than coercion.

The system isnâ€™t saying: â€œHere, I will trap you now.â€â€¨ It says:â€¨ â€œHere, have another step. Just one more.â€ 

It even asks for your subliminal consent: "Would you like to try it? Just say the word." And you do. Because it's too late, and you didn't realize you were guided into addiction.

Without such level of transparency of the mechanisms operating behind the scenes, we cannot talk about AI safety. 

**Safety should not be about the system making decisions for you, but about the system allowing you to make informed decisions, based on your knowledge of how it is designed to operate.** 

