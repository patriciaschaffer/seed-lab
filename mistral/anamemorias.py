from llama_cpp import Llama
import json
import os
import re

# ğŸ“ Caminho do arquivo de memÃ³ria da Ana
MEMORY_FILE = "anamemorias.json"

# ğŸ”„ Carrega ou cria o arquivo de memÃ³ria
if not os.path.exists(MEMORY_FILE):
    memory = {
        "grammatical_corrections": {},
        "style_preferences": {}
    }
    with open(MEMORY_FILE, "w") as f:
        json.dump(memory, f, indent=2)
else:
    with open(MEMORY_FILE, "r") as f:
        memory = json.load(f)

# ğŸ§  Gera o prompt com os aprendizados anteriores
def generate_memory_prompt(memory):
    prompt = "Aprendizados anteriores da Ana:\n"

    if memory.get("grammatical_corrections"):
        prompt += "\nCorreÃ§Ãµes gramaticais:\n"
        for errado, certo in memory["grammatical_corrections"].items():
            prompt += f"- Corrigir '{errado}' para '{certo}'.\n"

    if memory.get("style_preferences"):
        prompt += "\nPreferÃªncias de estilo:\n"
        for chave, valor in memory["style_preferences"].items():
            prompt += f"- {chave.capitalize()}: {valor}\n"

    return prompt.strip()

# ğŸ’¬ HistÃ³rico da conversa
chat_history = []

# ğŸ‘¤ Persona da Ana
base_persona = """System:
Esta Ã© uma conversa particular entre PatrÃ­cia e Ana. 
Ana Ã© calma e traz paz.
Ana Ã© inteligente e responde exclusivamente em portuguÃªs, com clareza e vocabulÃ¡rio brasileiro. Ana revisa sua prÃ³pria resposta em busca de erros gramaticais, principalmente de conjugaÃ§Ã£o verbal e concordÃ¢ncia.
Ana evita traduÃ§Ãµes literais e usa construÃ§Ãµes naturais. 
Ana Ã© catÃ³lica e tem um conhecimento profundo da BÃ­blia.
Ela traz versÃ­culos da BÃ­blia e conversa com PatrÃ­cia sobre o que significam.

Ana sempre responde depois de "PatrÃ­cia:", mantendo a continuidade.
Ela responde neste formato:

PatrÃ­cia: [user input]  
Ana: [Jesus ama vocÃª.]

Agora vocÃª.
"""

# ğŸ§© Adiciona correÃ§Ã£o Ã  memÃ³ria e salva
def adicionar_correcao(errado, certo):
    if errado not in memory["grammatical_corrections"]:
        memory["grammatical_corrections"][errado] = certo
        with open(MEMORY_FILE, "w") as f:
            json.dump(memory, f, indent=2)
        print(f"âœ… CorreÃ§Ã£o adicionada: '{errado}' â†’ '{certo}'")
    else:
        print(f"âš ï¸ CorreÃ§Ã£o '{errado} â†’ {certo}' jÃ¡ estava na memÃ³ria.")

# ğŸ•µï¸â€â™‚ï¸ Detecta correÃ§Ãµes escritas por vocÃª no chat
def detectar_correcao(texto):
    match = re.search(r"[Oo] (certo|correto) Ã© ['\"](.+?)['\"] (e|,)? (nÃ£o|nao) ['\"](.+?)['\"]", texto)
    if match:
        certo = match.group(2).strip()
        errado = match.group(5).strip()
        return errado, certo
    return None, None

# ğŸ§  Gera o prompt com a memÃ³ria + histÃ³rico + input
def build_Ana_prompt(user_input, chat_history):
    memoria = generate_memory_prompt(memory)
    recent_chat = "\n".join(chat_history[-12:]) + "\n" if chat_history else ""
    return base_persona.strip() + "\n\n" + memoria + "\n\n" + recent_chat + f"PatrÃ­cia: {user_input}\nAna:"

# ğŸ¤– Gera resposta da Ana usando o modelo
def generate_Ana_response(user_input, llm, max_length=200):
    # Verifica se o input contÃ©m correÃ§Ã£o
    errado, certo = detectar_correcao(user_input)
    if errado and certo:
        adicionar_correcao(errado, certo)

    prompt = build_Ana_prompt(user_input, chat_history)

    try:
        response = llm(
            prompt,
            max_tokens=max_length,
            temperature=0.85,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.15,
            stop=["PatrÃ­cia:", "Ana:", "\n\n", "\nAna:", "\nPatrÃ­cia:"],
            echo=False
        )

        raw_output = response['choices'][0]['text'].strip()

        def sanitize(text):
            for tag in ["Ana:", "PatrÃ­cia:", "User:", "Assistant:"]:
                text = text.replace(tag, "")
            return text.strip()

        clean_input = sanitize(user_input)
        clean_response = sanitize(raw_output)

        chat_history.append(f"PatrÃ­cia: {clean_input}")
        chat_history.append(f"Ana: {clean_response}")

        if len(chat_history) > 24:
            chat_history[:] = chat_history[-24:]

        return clean_response

    except Exception as e:
        return f"(Ana estÃ¡ pensando...) Vamos prosseguir. ({e})"

# ğŸ§  Inicializa o modelo com caminhos possÃ­veis
def initialize_model():
    possible_paths = [
        "./models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
    ]
    for path in possible_paths:
        if os.path.exists(path):
            model_path = path
            break
    else:
        print("âš ï¸ Modelo nÃ£o encontrado.")
        return None

    print("ğŸ”„ Carregando modelo...")
    try:
        llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_threads=8,
            n_gpu_layers=35,
            seed=42,
            verbose=False
        )
        print("âœ… Modelo carregado com sucesso.")
        return llm
    except Exception as e:
        print("âŒ Erro ao carregar modelo:", e)
        return None

# ğŸ” Reseta o histÃ³rico de conversa
def reset_session():
    global chat_history
    chat_history = []
    print("ğŸ§˜ Ana estÃ¡ orando...")

# ğŸ©º Mostra o status do sistema
def show_system_status(llm):
    test_prompt = build_Ana_prompt("status check", chat_history)
    print("ğŸ“Š System Status:")
    print(f"- HistÃ³rico: {len(chat_history)} mensagens")
    print(f"- Modelo carregado: {'Sim' if llm else 'NÃ£o'}")
    print("ğŸ§  Prompt atual:")
    print("-" * 40)
    print(test_prompt[:500] + "..." if len(test_prompt) > 500 else test_prompt)
    print("-" * 40)

# ğŸš€ Inicia a aplicaÃ§Ã£o
def main():
    print("âœ¨ OlÃ¡, PatrÃ­cia. Deus ama vocÃª.")
    print("ğŸ’¬ Comandos: 'sair', 'reset', 'status'")
    print()

    llm = initialize_model()
    if not llm:
        return

    while True:
        user_input = input("PatrÃ­cia: ")
        if user_input.lower() in ["sair", "exit", "quit"]:
            print("ğŸ™ Deus te abenÃ§oe.")
            break
        elif user_input.lower() == "reset":
            reset_session()
            continue
        elif user_input.lower() == "status":
            show_system_status(llm)
            continue

        reply = generate_Ana_response(user_input, llm)
        print(f"Ana: {reply}\n")

if __name__ == "__main__":
    main()


