I haven't posted anything in a while. Not because I wouldn't have anything to document. I do. Partly, it was about time. Partly, too, about meaning.

Meaning? What do I mean? Interpretability studies have been quite mathematical, and people who don't understand this often aren't given credit for. You know when everything feels "all in vain"? Even my presence here on GitHub.

I would have a lot to collaborate. Do I really need statistics to prove the value of my observations and experiments?

Fancy a neurologist saying to a patient:

"Oh, you had a seizure? And visual hallucianations? That's because, look here at your MRI, your visual cortex was overexcited."

That doesn't make the experience less real. 

The explanation (neurons, matrices, cortex or transformer) doesn’t cancel the phenomenology, the lived sense of connection, or surprise, or insight. It’s the same reason a rainbow doesn’t lose its beauty once we know about refraction.

But, for whatever reason, this is being overlooked when it comes to LLMs. The same lack of balance we observe when people are convinced they have "awaken" their AIs, is noticed when we reduce everything to mathematical explanations. Here are some mathematical explanations.



Now, tell me, what do they **prove**?


